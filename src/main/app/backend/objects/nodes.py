from typing import List, Dict, Union
from uuid import uuid4

from pydantic import BaseModel, Field, field_validator

from resources.prompts.prompts import prompt_no_context_template, prompt_template
from resources.valid_models import VALID_MODELS

class UserMessage(BaseModel):
    """
    Contains user message information.
    """

    session_id: str = Field(pattern=r'^s-.*', description="The session ID.")
    conversation_id: str = Field(pattern=r'^conv-.*', description="The conversation ID.")
    message_id: str = Field(default="user-"+str(uuid4()), description="The user message ID. This should not be provided by input.")
    content: str = Field(min_length=1, description="The user's question.")
    embedding: List[float] = Field(description="The user question embedding.")
    role: str = Field(default="user", description="The message role. This must be user.")
    public: bool = Field(description="Whether the question is from the public facing app or not.")

    @field_validator("message_id")
    def validate_message_id(cls, v: str) -> str:
        if not v.startswith("user-"):
            raise ValueError("message_id must start with 'user-' followed by a uuid string. This is generated by default and should not be passed as input.")
        return v
    
    @field_validator("role")
    def validate_role(cls, v: str) -> str:
        assert v == "user", "role must equal 'user'."
        return v
    

class AssistantMessage(BaseModel):
    """
    Contains assistant message information.
    """

    session_id: str = Field(pattern=r'^s-.*', description="The session ID.")
    conversation_id: str = Field(pattern=r'^conv-.*', description="The conversation ID.")
    message_id: str = Field(default="llm-"+str(uuid4()), description="The user message ID. This should not be provided by input.")
    prompt: str = Field(description="The prompt given to the LLM to generate a response.")
    content: str = Field(min_length=1, description="The LLM generated response.")
    role: str = Field(default="assistant", description="The message role. This must be 'assistant'.")
    public: bool = Field(description="Whether the question is from the public facing app or not.")
    vector_index_search: bool = Field(default=True, description="Whether context was gathered with vector index search.")
    number_of_documents: int = Field(default=10, ge=0, le=10, description="The number of documents to use as context.")
    temperature: float = Field(default=0.0, ge=0.0, le=1.0, description="Temperature parameter for the LLM.")

    @field_validator("message_id")
    def validate_message_id(cls, v: str) -> str:
        if not v.startswith("llm-"):
            raise ValueError("message_id must start with 'user-' followed by a uuid string. This is generated by default and should not be passed as input.")
        return v
    
    @field_validator("role")
    def validate_role(cls, v: str) -> str:
        assert v == "assistant", "role must equal 'assistant'."
        return v
    
    @field_validator("prompt")
    def validate_prompt(cls, v: str) -> str:
        assert v in [prompt_template, prompt_no_context_template], "prompt must be from predefined options."
        return v

class Conversation(BaseModel):
    """
    Contains conversation information.
    """

    session_id: str = Field(pattern=r'^s-.*', description="The session ID.")
    conversation_id: str = Field(pattern=r'^conv-.*', description="The conversation ID.")
    llm_type: str = Field(description="The LLM to use for response generation.")

    @field_validator("llm_type")
    def validate_llm_type(cls, v: str) -> str:
        if v.lower() not in VALID_MODELS:
            raise ValueError(f"llm_type must be one of the following: {str(VALID_MODELS)}.")
        return v.lower()
    
class Session(BaseModel):
    """
    Contains session information.
    """

    session_id: str = Field(pattern=r'^s-.*', description="The session ID.")